{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594d4525",
   "metadata": {},
   "source": [
    "# InceptionTime Pytorch Implemenation\n",
    "Septemeber 23rd 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e0ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet model\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# from utils.utils import save_logs\n",
    "# from utils.utils import calculate_metrics\n",
    "# from utils.utils import save_test_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0291fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def correct_sizes(sizes):\n",
    "\tcorrected_sizes = [s if s % 2 != 0 else s - 1 for s in sizes]\n",
    "\treturn corrected_sizes\n",
    "\n",
    "\n",
    "def pass_through(X):\n",
    "\treturn X\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU(), return_indices=False):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t: param return_indices\t\t\tIndices are needed only if we want to create decoder with InceptionTranspose with MaxUnpool1d. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Inception, self).__init__()\n",
    "\t\tself.return_indices=return_indices\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_pool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1, return_indices=return_indices)\n",
    "\t\tself.conv_from_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=4*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ_maxpool, indices = self.max_pool(X)\n",
    "\t\telse:\n",
    "\t\t\tZ_maxpool = self.max_pool(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\tZ4 = self.conv_from_maxpool(Z_maxpool)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3, Z4], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z, indices\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "\t\tsuper(InceptionBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.return_indices = return_indices\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=4*n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=4*n_filters\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ, i1 = self.inception_1(X)\n",
    "\t\t\tZ, i2 = self.inception_2(Z)\n",
    "\t\t\tZ, i3 = self.inception_3(Z)\n",
    "\t\telse:\n",
    "\t\t\tZ = self.inception_1(X)\n",
    "\t\t\tZ = self.inception_2(Z)\n",
    "\t\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z,[i1, i2, i3]\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "\n",
    "class InceptionTranspose(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU()):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(InceptionTranspose, self).__init__()\n",
    "\t\tself.activation = activation\n",
    "\t\tself.conv_to_bottleneck_1 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_2 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_3 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_unpool = nn.MaxUnpool1d(kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=3*bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "\n",
    "\t\tdef forward(self, X, indices):\n",
    "\t\t\tZ1 = self.conv_to_bottleneck_1(X)\n",
    "\t\t\tZ2 = self.conv_to_bottleneck_2(X)\n",
    "\t\t\tZ3 = self.conv_to_bottleneck_3(X)\n",
    "\t\t\tZ4 = self.conv_to_maxpool(X)\n",
    "\n",
    "\t\t\tZ = torch.cat([Z1, Z2, Z3], axis=1)\n",
    "\t\t\tMUP = self.max_unpool(Z4, indices)\n",
    "\t\t\tBN = self.bottleneck(Z)\n",
    "\t\t\t# another possibility insted of sum BN and MUP is adding 2nd bottleneck transposed convolution\n",
    "\t\t\t\n",
    "\t\t\treturn self.activation(self.batch_norm(BN + MUP))\n",
    "\n",
    "\n",
    "class InceptionTransposeBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU()):\n",
    "\t\tsuper(InceptionTransposeBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=out_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=out_channels\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X, indices):\n",
    "\t\tassert len(indices)==3\n",
    "\t\tZ = self.inception_1(X, indices[2])\n",
    "\t\tZ = self.inception_2(Z, indices[1])\n",
    "\t\tZ = self.inception_3(Z, indices[0])\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\treturn Z\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\tdef __init__(self, out_features):\n",
    "\t\tsuper(Flatten, self).__init__()\n",
    "\t\tself.output_dim = out_features\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(-1, self.output_dim)\n",
    "    \n",
    "class Reshape(nn.Module):\n",
    "\tdef __init__(self, out_shape):\n",
    "\t\tsuper(Reshape, self).__init__()\n",
    "\t\tself.out_shape = out_shape\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(-1, *self.out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356730c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "InceptionTime = nn.Sequential(\n",
    "                    Reshape(out_shape=(56,100)),\n",
    "                    InceptionBlock(\n",
    "                        in_channels=56, \n",
    "                        n_filters=32, \n",
    "                        kernel_sizes=[5, 11, 23],\n",
    "                        bottleneck_channels=32,\n",
    "                        use_residual=True,\n",
    "                        activation=nn.ReLU()\n",
    "                    ),\n",
    "                    InceptionBlock(\n",
    "                        in_channels=32*4, \n",
    "                        n_filters=32, \n",
    "                        kernel_sizes=[5, 11, 23],\n",
    "                        bottleneck_channels=32,\n",
    "                        use_residual=True,\n",
    "                        activation=nn.ReLU()\n",
    "                    ),\n",
    "                    nn.AdaptiveAvgPool1d(output_size=1),\n",
    "                    Flatten(out_features=32*4*1),\n",
    "                    nn.Linear(in_features=4*32*1, out_features=3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ebb54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_with_auxillary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Inception_with_auxillary, self).__init__()\n",
    "        \n",
    "        self.reshape = Reshape(out_shape=(56,1500))\n",
    "        self.block_i = InceptionBlock(\n",
    "            in_channels=56, \n",
    "            n_filters=32, \n",
    "            kernel_sizes=[5, 11, 23],\n",
    "            bottleneck_channels=32,\n",
    "            use_residual=True,\n",
    "            activation=nn.ReLU())\n",
    "        \n",
    "        self.block = InceptionBlock(\n",
    "            in_channels=32*4, \n",
    "            n_filters=32, \n",
    "            kernel_sizes=[5, 11, 23],\n",
    "            bottleneck_channels=32,\n",
    "            use_residual=True,\n",
    "            activation=nn.ReLU())\n",
    "        \n",
    "        self.auxillary_out = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool1d(output_size=1),\n",
    "                    Flatten(out_features=32*4*1),\n",
    "                    nn.Linear(in_features=4*32*1, out_features=3))\n",
    "        \n",
    "    def forward(self,input_mat):\n",
    "        # Initial Layers\n",
    "        resized_input_mat = self.reshape(input_mat)\n",
    "        Inception_out_1 = self.block_i(resized_input_mat) # [5, 11, 23]\n",
    "        Inception_out_2 = self.block(Inception_out_1) # [5, 11, 23]\n",
    "\n",
    "        # Auxillary 1\n",
    "        aux_1 = self.auxillary_out(Inception_out_2)\n",
    "\n",
    "        # Deep Blocks 1\n",
    "        Inception_out_3 = self.block(Inception_out_2) # [5, 11, 23]\n",
    "        Inception_out_4 = self.block(Inception_out_3) # [5, 11, 23]\n",
    "\n",
    "        # Auxillary 2\n",
    "        aux_2 = self.auxillary_out(Inception_out_2)\n",
    "\n",
    "        # Deep Blocks 1\n",
    "        Inception_out_5 = self.block(Inception_out_4) # [5, 11, 23]\n",
    "        Inception_out_6 = self.block(Inception_out_5) # [5, 11, 23]\n",
    "\n",
    "        # Final Out\n",
    "        main = self.auxillary_out(Inception_out_6)\n",
    "\n",
    "#         aux_1 = nn.Softmax(aux_1)\n",
    "#         aux_2 = nn.Softmax(aux_2)\n",
    "#         main = nn.Softmax(main)\n",
    "\n",
    "        out_arr = [aux_1, aux_2, main]\n",
    "    \n",
    "        return(out_arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12e0bd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[5, 11, 23], [5, 11, 23]],\n",
       " [[5, 11, 23], [5, 11, 23]],\n",
       " [[5, 11, 23], [5, 11, 23]]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [[[5, 11, 23],[5, 11, 23]],[[5, 11, 23],[5, 11, 23]],[[5, 11, 23],[5, 11, 23]]]\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bbb0c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list(Inception_with_auxillary().named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af23a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Reshape(out_shape=(56,100))\n",
    "block_1 = InceptionBlock(\n",
    "    in_channels=56, \n",
    "    n_filters=32, \n",
    "    kernel_sizes=[5, 11, 23],\n",
    "    bottleneck_channels=32,\n",
    "    use_residual=True,\n",
    "    activation=nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f94f437f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(56, 100)\n",
    "x = r(x)\n",
    "x = block_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78097601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0896, 0.1675, 0.7607,  ..., 0.3851, 1.9367, 1.0594],\n",
       "         [0.0000, 2.3273, 1.4003,  ..., 1.1813, 0.0000, 0.4576],\n",
       "         [0.4122, 0.0000, 1.4269,  ..., 0.0000, 1.0406, 2.6926],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 1.5164, 0.0000, 0.0000],\n",
       "         [0.1700, 1.0429, 1.6943,  ..., 0.8134, 0.4646, 0.0000],\n",
       "         [1.7511, 1.4403, 2.4411,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bae55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
