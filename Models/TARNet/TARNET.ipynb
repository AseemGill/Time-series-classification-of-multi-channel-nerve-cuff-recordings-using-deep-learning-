{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b77fd4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import TAR_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7504580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndevice = 'cuda:2'    \\nlr, dropout = 0.01, 0.01\\nnclasses, seq_len, batch, input_size = 12, 5, 11, 10\\nemb_size, nhid, nhead, nlayers = 32, 128, 2, 3\\nnhid_tar, nhid_task = 128, 128\\ntask_type = 'regression'\\nmodel = MultitaskTransformerModel(task_type, device, nclasses, seq_len, batch, input_size, emb_size, nhead, nhid, nhid_tar, nhid_task, nlayers, dropout = 0.1).to(device)\\nx = torch.randn(batch, seq_len, input_size) * 50\\nx = torch.as_tensor(x).float()\\nprint(x.shape)\\n(out_tar, attn_tar), (out_task, attn_task) = model(torch.as_tensor(x, device = device), 'reconstruction'), model(torch.as_tensor(x, device = device), task_type)\\nprint(out_tar.shape)\\nprint(attn_tar.shape)\\nprint(out_task.shape)\\nprint(attn_task.shape)\\n\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, d_model, dropout = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        max_len = max(5000, seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[: , 0 : -1]\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    # Input: seq_len x batch_size x dim, Output: seq_len, batch_size, dim\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Permute(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "class MultitaskTransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, task_type, device, nclasses, seq_len, batch, input_size, emb_size, nhead, nhid, nhid_tar, nhid_task, nlayers, dropout = 0.1):\n",
    "        super(MultitaskTransformerModel, self).__init__()\n",
    "        # from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        \n",
    "        self.trunk_net = nn.Sequential(\n",
    "            nn.Linear(input_size, emb_size),\n",
    "#             nn.BatchNorm1d(batch),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            PositionalEncoding(seq_len, emb_size, dropout),\n",
    "            nn.LayerNorm(emb_size),\n",
    "#             nn.BatchNorm1d(batch)\n",
    "        )\n",
    "        \n",
    "        # encoder_layers = transformer_encoder_class.TransformerEncoderLayer(emb_size, nhead, nhid, out_channel, filter_height, filter_width, dropout)\n",
    "        # encoder_layers = TransformerEncoderLayer(emb_size, nhead, nhid, dropout)\n",
    "        # self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        encoder_layers = TAR_transformer.TransformerEncoderLayer(emb_size, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TAR_transformer.TransformerEncoder(encoder_layers, nlayers, device)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(batch)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(emb_size)\n",
    "        \n",
    "        # Task-aware Reconstruction Layers\n",
    "        self.tar_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, nhid_tar),\n",
    "            nn.BatchNorm1d(batch),\n",
    "            nn.Linear(nhid_tar, nhid_tar),\n",
    "            nn.BatchNorm1d(batch),\n",
    "            nn.Linear(nhid_tar, input_size),\n",
    "        )\n",
    "\n",
    "        if task_type == 'classification':\n",
    "            # Classification Layers\n",
    "            self.class_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "#                 nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Dropout(p = 0.3),\n",
    "                nn.Linear(nhid_task, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "#                 nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Dropout(p = 0.3),\n",
    "                nn.Linear(nhid_task, nclasses)\n",
    "            )\n",
    "        else:\n",
    "            # Regression Layers\n",
    "            self.reg_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "                nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Linear(nhid_task, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "                nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Linear(nhid_task, 1),\n",
    "            )\n",
    "            \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = self.trunk_net(x.permute(2, 0, 1))\n",
    "        x, attn = self.transformer_encoder(x)\n",
    "#         x = self.batch_norm(x)\n",
    "        # x : seq_len x batch x emb_size\n",
    "        output = self.class_net(x[-1])\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0f0f883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_type = \"classification\"\n",
    "device = \"cuda\"\n",
    "nclasses = 3\n",
    "seq_len = 1500\n",
    "batch_size = 64\n",
    "input_size = 1500\n",
    "emb_size = 512\n",
    "nhead = 8\n",
    "nhid = 64\n",
    "nhid_tar = 1024\n",
    "nhid_task = 128\n",
    "nlayers = 2\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 80\n",
    "use_cuda = True\n",
    "early_stop = 3\n",
    "min_delta = -0.025\n",
    "min_epochs = 30\n",
    "num_workers = 8\n",
    "\n",
    "model = MultitaskTransformerModel(task_type, device, nclasses, seq_len, batch, input_size, emb_size, nhead, nhid, nhid_tar, nhid_task, nlayers, dropout = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ab58145",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"Rat2_DF_2.pt\")\n",
    "x = x.unsqueeze(0)\n",
    "x = x.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8aff376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 56])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4c4d034a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_8492\\1832034341.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2644,  0.0107, -0.0934]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[[0.0355, 0.0453, 0.0484,  ..., 0.0236, 0.0283, 0.0341],\n",
       "          [0.0254, 0.0365, 0.0385,  ..., 0.0302, 0.0337, 0.0319],\n",
       "          [0.0270, 0.0385, 0.0390,  ..., 0.0342, 0.0402, 0.0451],\n",
       "          ...,\n",
       "          [0.0314, 0.0388, 0.0406,  ..., 0.0420, 0.0411, 0.0381],\n",
       "          [0.0358, 0.0336, 0.0420,  ..., 0.0361, 0.0312, 0.0341],\n",
       "          [0.0306, 0.0337, 0.0389,  ..., 0.0391, 0.0324, 0.0357]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52b034c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 1, 1500])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.permute(2, 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcfff6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.permute>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c7e81e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultitaskTransformerModel(\n",
       "  (trunk_net): Sequential(\n",
       "    (0): Linear(in_features=1500, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (tar_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Linear(in_features=1024, out_features=1500, bias=True)\n",
       "  )\n",
       "  (class_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Permute()\n",
       "    (3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Permute()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Permute()\n",
       "    (9): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Permute()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ee03e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae1a3b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of MultitaskTransformerModel(\n",
       "  (trunk_net): Sequential(\n",
       "    (0): Linear(in_features=1500, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (tar_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Linear(in_features=1024, out_features=1500, bias=True)\n",
       "  )\n",
       "  (class_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Permute()\n",
       "    (3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Permute()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Permute()\n",
       "    (9): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Permute()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ac792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
