{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7df998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\Anaconda3\\envs\\Aseem\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46910a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionEncoder, self).__init__()\n",
    "        \n",
    "        # Define Queries, Keys, and Values\n",
    "        self.linear_q = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Multi Head Attention\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=1, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Obtain Queries, Keys, and Values\n",
    "        q, k, v = self.linear_q(x), self.linear_k(x), self.linear_v(x)\n",
    "        \n",
    "        # Compute Attention Matrix\n",
    "        x = self.attention(q,k,v)\n",
    "        return x\n",
    "\n",
    "class MSCB(nn.Module):\n",
    "    def __init__(self, small_kernel, medium_kernel, large_kernel, num_filters, hidden_size):\n",
    "        super(MSCB, self).__init__()\n",
    "        self.name = \"MSCB\"\n",
    "\n",
    "        # Define Small Path\n",
    "        self.convS = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = small_kernel, padding = 'same')\n",
    "        self.MPoolS = nn.MaxPool1d(kernel_size = small_kernel, stride = 5, padding = int(small_kernel/2 - 1))\n",
    "        \n",
    "        # Define Medium Path\n",
    "        self.convM = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = medium_kernel, padding = 'same')\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = medium_kernel, stride = 5, padding = int(medium_kernel/2 - 1))\n",
    "        \n",
    "        # Define Large Path\n",
    "        self.convL = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = large_kernel, padding = 'same')\n",
    "        self.MPoolL = nn.MaxPool1d(kernel_size = large_kernel, stride = 5, padding = int(large_kernel/2 - 1))\n",
    "        \n",
    "        # Define MaxPool First\n",
    "        self.conv = nn.Conv1d(in_channels = 56, out_channels = 128, kernel_size = 24, padding = 'same')\n",
    "        self.MPool = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels = 3*num_filters + 128, out_channels = 64, kernel_size = 112, padding = 'same')\n",
    "        self.MPool2 = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.conv3 = nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 25, padding = 'same')\n",
    "        \n",
    "        # Define Multi-Head Attention\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = AttentionEncoder(input_size  = 3840, hidden_size = hidden_size)\n",
    "        self.fc1 = nn.Linear(in_features = hidden_size, out_features = 1024)\n",
    "        self.fc2 = nn.Linear(in_features = 1024, out_features = 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_size = self.hidden_size\n",
    "        ### Feature Learning Head\n",
    "        \n",
    "        # Ensure Batch is first and data type is correct\n",
    "        x = torch.moveaxis(x,2,1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        # Parrallel Convolutions\n",
    "        x_S = self.MPoolS(F.relu(self.convS(x)))\n",
    "        x_M = self.MPoolM(F.relu(self.convM(x)))\n",
    "        x_L = self.MPoolL(F.relu(self.convL(x)))\n",
    "        x_O = F.relu(self.conv(self.MPool(x)))\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat((x_S,x_M,x_L,x_O),1)\n",
    "        \n",
    "        # Post Concatenation\n",
    "        x = self.MPool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "                \n",
    "        # Flattening\n",
    "        x = x.view(-1,3840)\n",
    "        \n",
    "        # Compute Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Flattening\n",
    "        x = x[0]\n",
    "        x = x.view(-1,hidden_size)\n",
    "        \n",
    "        #Classification Head\n",
    "        x = F.relu(self.fc1((x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c8c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
