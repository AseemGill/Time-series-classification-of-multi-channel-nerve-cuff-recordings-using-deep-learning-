{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922765cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\.conda\\envs\\Aseem\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import TAR_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d7544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sizes(sizes):\n",
    "\tcorrected_sizes = [s if s % 2 != 0 else s - 1 for s in sizes]\n",
    "\treturn corrected_sizes\n",
    "\n",
    "\n",
    "def pass_through(X):\n",
    "\treturn X\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU(), return_indices=False):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t: param return_indices\t\t\tIndices are needed only if we want to create decoder with InceptionTranspose with MaxUnpool1d. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Inception, self).__init__()\n",
    "\t\tself.return_indices=return_indices\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_pool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1, return_indices=return_indices)\n",
    "\t\tself.conv_from_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=4*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ_maxpool, indices = self.max_pool(X)\n",
    "\t\telse:\n",
    "\t\t\tZ_maxpool = self.max_pool(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\tZ4 = self.conv_from_maxpool(Z_maxpool)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3, Z4], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z, indices\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "\t\tsuper(InceptionBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.return_indices = return_indices\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=4*n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=4*n_filters\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ, i1 = self.inception_1(X)\n",
    "\t\t\tZ, i2 = self.inception_2(Z)\n",
    "\t\t\tZ, i3 = self.inception_3(Z)\n",
    "\t\telse:\n",
    "\t\t\tZ = self.inception_1(X)\n",
    "\t\t\tZ = self.inception_2(Z)\n",
    "\t\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z,[i1, i2, i3]\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "\n",
    "class InceptionTranspose(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU()):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(InceptionTranspose, self).__init__()\n",
    "\t\tself.activation = activation\n",
    "\t\tself.conv_to_bottleneck_1 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_2 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_3 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_unpool = nn.MaxUnpool1d(kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=3*bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "\n",
    "\t\tdef forward(self, X, indices):\n",
    "\t\t\tZ1 = self.conv_to_bottleneck_1(X)\n",
    "\t\t\tZ2 = self.conv_to_bottleneck_2(X)\n",
    "\t\t\tZ3 = self.conv_to_bottleneck_3(X)\n",
    "\t\t\tZ4 = self.conv_to_maxpool(X)\n",
    "\n",
    "\t\t\tZ = torch.cat([Z1, Z2, Z3], axis=1)\n",
    "\t\t\tMUP = self.max_unpool(Z4, indices)\n",
    "\t\t\tBN = self.bottleneck(Z)\n",
    "\t\t\t# another possibility insted of sum BN and MUP is adding 2nd bottleneck transposed convolution\n",
    "\t\t\t\n",
    "\t\t\treturn self.activation(self.batch_norm(BN + MUP))\n",
    "\n",
    "\n",
    "class InceptionTransposeBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU()):\n",
    "\t\tsuper(InceptionTransposeBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=out_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=out_channels\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X, indices):\n",
    "\t\tassert len(indices)==3\n",
    "\t\tZ = self.inception_1(X, indices[2])\n",
    "\t\tZ = self.inception_2(Z, indices[1])\n",
    "\t\tZ = self.inception_3(Z, indices[0])\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\treturn Z\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\tdef __init__(self, out_features):\n",
    "\t\tsuper(Flatten, self).__init__()\n",
    "\t\tself.output_dim = out_features\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(-1, self.output_dim)\n",
    "    \n",
    "class Reshape(nn.Module):\n",
    "\tdef __init__(self, out_shape):\n",
    "\t\tsuper(Reshape, self).__init__()\n",
    "\t\tself.out_shape = out_shape\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(-1, *self.out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2e7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_with_auxillary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Inception_with_auxillary, self).__init__()\n",
    "        \n",
    "        self.reshape = Reshape(out_shape=(56,1500))\n",
    "        self.block_i = InceptionBlock(\n",
    "            in_channels=56, \n",
    "            n_filters=32, \n",
    "            kernel_sizes=[5, 11, 23],\n",
    "            bottleneck_channels=32,\n",
    "            use_residual=True,\n",
    "            activation=nn.ReLU())\n",
    "        \n",
    "        self.block = InceptionBlock(\n",
    "            in_channels=32*4, \n",
    "            n_filters=32, \n",
    "            kernel_sizes=[5, 11, 23],\n",
    "            bottleneck_channels=32,\n",
    "            use_residual=True,\n",
    "            activation=nn.ReLU())\n",
    "        \n",
    "        self.auxillary_out = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool1d(output_size=1),\n",
    "                    Flatten(out_features=32*4*1),\n",
    "                    nn.Linear(in_features=4*32*1, out_features=3))\n",
    "        \n",
    "    def forward(self,input_mat):\n",
    "        # Initial Layers\n",
    "        resized_input_mat = self.reshape(input_mat)\n",
    "        Inception_out_1 = self.block_i(resized_input_mat)\n",
    "        Inception_out_2 = self.block(Inception_out_1)\n",
    "\n",
    "        # Auxillary 1\n",
    "        aux_1 = self.auxillary_out(Inception_out_2)\n",
    "\n",
    "        # Deep Blocks 1\n",
    "        Inception_out_3 = self.block(Inception_out_2)\n",
    "        Inception_out_4 = self.block(Inception_out_3)\n",
    "\n",
    "        # Auxillary 2\n",
    "        aux_2 = self.auxillary_out(Inception_out_2)\n",
    "\n",
    "        # Deep Blocks 1\n",
    "        Inception_out_5 = self.block(Inception_out_4)\n",
    "        Inception_out_6 = self.block(Inception_out_5)\n",
    "\n",
    "        # Final Out\n",
    "        main = self.auxillary_out(Inception_out_6)\n",
    "\n",
    "#         aux_1 = nn.Softmax(aux_1)\n",
    "#         aux_2 = nn.Softmax(aux_2)\n",
    "#         main = nn.Softmax(main)\n",
    "\n",
    "        out_arr = [aux_1, aux_2, main]\n",
    "    \n",
    "        return(out_arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd4f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCB(nn.Module):\n",
    "    def __init__(self, small_kernel, medium_kernel, large_kernel, num_filters):\n",
    "        super(MSCB, self).__init__()\n",
    "        self.name = \"MSCB\"\n",
    "\n",
    "        # Define Small Path\n",
    "        self.convS = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = small_kernel, padding = 'same')\n",
    "        self.MPoolS = nn.MaxPool1d(kernel_size = small_kernel, stride = 5, padding = int(small_kernel/2 - 1))\n",
    "        \n",
    "        # Define Medium Path\n",
    "        self.convM = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = medium_kernel, padding = 'same')\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = medium_kernel, stride = 5, padding = int(medium_kernel/2 - 1))\n",
    "        \n",
    "        # Define Large Path\n",
    "        self.convL = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = large_kernel, padding = 'same')\n",
    "        self.MPoolL = nn.MaxPool1d(kernel_size = large_kernel, stride = 5, padding = int(large_kernel/2 - 1))\n",
    "        \n",
    "        #\n",
    "        self.MPool = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.conv = nn.Conv1d(in_channels = 56, out_channels = 128, kernel_size = 24, padding = 'same')\n",
    "        \n",
    "        #\n",
    "        self.conv2 = nn.Conv1d(in_channels = 3*num_filters + 128, out_channels = 64, kernel_size = 112, padding = 'same')\n",
    "        self.MPool2 = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.fc1 = nn.Linear(in_features = 5760, out_features = 400)\n",
    "        self.Dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features = 400, out_features = 1024)\n",
    "        self.fc3 = nn.Linear(in_features = 1024, out_features = 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature Learning Head\n",
    "        \n",
    "        x = torch.moveaxis(x,2,1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        x_S = self.MPoolS(F.relu(self.convS(x)))\n",
    "        x_M = self.MPoolM(F.relu(self.convM(x)))\n",
    "        x_L = self.MPoolL(F.relu(self.convL(x)))\n",
    "        x_O = F.relu(self.conv(self.MPool(x)))\n",
    "        \n",
    "#         print(x_S.shape)\n",
    "#         print(x_M.shape)\n",
    "#         print(x_L.shape)\n",
    "#         print(x_O.shape)\n",
    "        \n",
    "        #\n",
    "        x = torch.cat((x_S,x_M,x_L,x_O),1)\n",
    "        x = self.MPool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flattening\n",
    "        x = x.view(-1,5760)\n",
    "\n",
    "        #Classification Head\n",
    "        x = F.relu(self.fc1((x)))\n",
    "        x = self.Dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fcc84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR SETTING UP DATA LOADERS\n",
    "def prepare_valtest_dataset(data_dir, batch_size =  64, num_workers = 0):\n",
    "\n",
    "    # Specific where the folder containing the images is for the specific dataset\n",
    "    # This function contains sub-directories each containing all images in a \n",
    "    # single class\n",
    "    data = torchvision.datasets.DatasetFolder(data_dir, loader = torch.load, extensions = \".pt\")#, transform=data_transform)\n",
    "    \n",
    "\n",
    "    # prepare data loaders\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, \n",
    "                                            num_workers=num_workers, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "### FUNCTION FOR SETTING UP DATA LOADERS\n",
    "def prepare_train_dataset(data_dir, batch_size =  64, num_workers = 0):\n",
    "\n",
    "    # Specific where the folder containing the images is for the specific dataset\n",
    "    # This function contains sub-directories each containing all images in a \n",
    "    # single class\n",
    "    data1 = torchvision.datasets.DatasetFolder(data_dir[0], loader = torch.load, extensions = \".pt\")\n",
    "    data2 = torchvision.datasets.DatasetFolder(data_dir[1], loader = torch.load, extensions = \".pt\")\n",
    "    train_data = torch.utils.data.ConcatDataset((data1,data2))\n",
    "\n",
    "    # prepare data loaders\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                            num_workers=num_workers, shuffle=True)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6293de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = \"M:\\Peripheral Nerve Studies\\MCC Projects\\Aseem G\\Models\\Pytorch\\Data\\Spike Firing Rate\\Minimum Dataset\\Debug_Rat\\\\\"\n",
    "# fold1_dir = base_dir + \"Fold1\"\n",
    "# fold2_dir = base_dir + \"Fold2\"\n",
    "# fold3_dir = base_dir + \"Fold3\"\n",
    "# test_dir = base_dir + \"Test\"\n",
    "\n",
    "def three_fold_cross_sets(base_dir,fold1_dir,fold2_dir,fold3_dir,test_dir, batch_size =  64, num_workers = 0):\n",
    "    #Fold 1\n",
    "    train_set_1 = prepare_train_dataset([fold1_dir,fold2_dir], batch_size, num_workers)\n",
    "    valid_set_1 = prepare_valtest_dataset(fold3_dir, batch_size, num_workers)\n",
    "\n",
    "    #Fold 2\n",
    "\n",
    "    train_set_2 = prepare_train_dataset([fold1_dir,fold3_dir], batch_size, num_workers)\n",
    "    valid_set_2 = prepare_valtest_dataset(fold2_dir, batch_size, num_workers)\n",
    "\n",
    "    #Fold 3\n",
    "    train_set_3 = prepare_train_dataset([fold2_dir,fold3_dir],batch_size, num_workers)\n",
    "    valid_set_3 = prepare_valtest_dataset(fold1_dir, batch_size, num_workers)\n",
    "\n",
    "    test_set = prepare_valtest_dataset(test_dir, batch_size, num_workers)\n",
    "    \n",
    "    return train_set_1, valid_set_1, train_set_2, valid_set_2, train_set_3, valid_set_3, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8b9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(array):\n",
    "    total_samples = np.sum(array)/1.\n",
    "    correct = np.sum(np.multiply(array,np.eye(3)))\n",
    "\n",
    "    accuracy = round(correct/total_samples*100,2)\n",
    "    return(accuracy)\n",
    "def recall(array, index):\n",
    "    num = array[index,index]\n",
    "    den = np.sum(array[index,:])\n",
    "    \n",
    "    return(num/den)\n",
    "\n",
    "def precision(array, index):\n",
    "    num = array[index,index]\n",
    "    den = np.sum(array[:,index])\n",
    "    \n",
    "    return(num/den)\n",
    "def macro_f1(array):\n",
    "    recall_arr = []\n",
    "    precision_arr = []\n",
    "\n",
    "    for i in range(3):\n",
    "        recall_arr.append(recall(array, i))\n",
    "        precision_arr.append(precision(array, i))\n",
    "        \n",
    "    per_class_f1 = []\n",
    "    for i in range(3):\n",
    "        per_class_f1.append((2*recall_arr[i]*precision_arr[i])/(recall_arr[i] + precision_arr[i]))\n",
    "        \n",
    "    f1 = sum(per_class_f1)/3\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d490fa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1aac21b98d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89fcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1500 in\n",
    "\n",
    "\n",
    "# class MSCB(nn.Module):\n",
    "#     def __init__(self, small_kernel, medium_kernel, large_kernel, num_filters):\n",
    "#         super(MSCB, self).__init__()\n",
    "#         self.name = \"MSCB\"\n",
    "\n",
    "#         # Define Small Path\n",
    "#         self.convS = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = small_kernel, padding = 'same')\n",
    "#         self.MPoolS = nn.MaxPool1d(kernel_size = small_kernel, stride = 5, padding = int(small_kernel/2 - 1))\n",
    "        \n",
    "#         # Define Medium Path\n",
    "#         self.convM = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = medium_kernel, padding = 'same')\n",
    "#         self.MPoolM = nn.MaxPool1d(kernel_size = medium_kernel, stride = 5, padding = int(medium_kernel/2 - 1))\n",
    "        \n",
    "#         # Define Large Path\n",
    "#         self.convL = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = large_kernel, padding = 'same')\n",
    "#         self.MPoolL = nn.MaxPool1d(kernel_size = large_kernel, stride = 5, padding = int(large_kernel/2 - 1))\n",
    "        \n",
    "#         #\n",
    "#         self.MPool = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "#         self.conv = nn.Conv1d(in_channels = 56, out_channels = 128, kernel_size = 24, padding = 'same')\n",
    "        \n",
    "#         #\n",
    "#         self.conv2 = nn.Conv1d(in_channels = 3*num_filters + 128, out_channels = 64, kernel_size = 112, padding = 'same')\n",
    "#         self.MPool2 = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "#         self.fc1 = nn.Linear(in_features = 3840, out_features = 400)\n",
    "#         self.Dropout = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(in_features = 400, out_features = 1024)\n",
    "#         self.fc3 = nn.Linear(in_features = 1024, out_features = 3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Feature Learning Head\n",
    "        \n",
    "#         x = torch.moveaxis(x,2,1)\n",
    "#         x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "#         x_S = self.MPoolS(F.relu(self.convS(x)))\n",
    "#         x_M = self.MPoolM(F.relu(self.convM(x)))\n",
    "#         x_L = self.MPoolL(F.relu(self.convL(x)))\n",
    "#         x_O = F.relu(self.conv(self.MPool(x)))\n",
    "        \n",
    "# #         print(x_S.shape)\n",
    "# #         print(x_M.shape)\n",
    "# #         print(x_L.shape)\n",
    "# #         print(x_O.shape)\n",
    "        \n",
    "#         #\n",
    "#         x = torch.cat((x_S,x_M,x_L,x_O),1)\n",
    "#         x = self.MPool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "#         # Flattening\n",
    "#         x = x.view(-1,3840)\n",
    "\n",
    "#         #Classification Head\n",
    "#         x = F.relu(self.fc1((x)))\n",
    "#         x = self.Dropout(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02553f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sizes(sizes):\n",
    "\tcorrected_sizes = [s if s % 2 != 0 else s - 1 for s in sizes]\n",
    "\treturn corrected_sizes\n",
    "\n",
    "\n",
    "def pass_through(X):\n",
    "\treturn X\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU(), return_indices=False):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t: param return_indices\t\t\tIndices are needed only if we want to create decoder with InceptionTranspose with MaxUnpool1d. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Inception, self).__init__()\n",
    "\t\tself.return_indices=return_indices\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_pool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1, return_indices=return_indices)\n",
    "\t\tself.conv_from_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=4*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ_maxpool, indices = self.max_pool(X)\n",
    "\t\telse:\n",
    "\t\t\tZ_maxpool = self.max_pool(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\tZ4 = self.conv_from_maxpool(Z_maxpool)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3, Z4], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "# \t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z, indices\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "\t\tsuper(InceptionBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.return_indices = return_indices\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=4*n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=4*n_filters\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ, i1 = self.inception_1(X)\n",
    "\t\t\tZ, i2 = self.inception_2(Z)\n",
    "\t\t\tZ, i3 = self.inception_3(Z)\n",
    "\t\telse:\n",
    "\t\t\tZ = self.inception_1(X)\n",
    "\t\t\tZ = self.inception_2(Z)\n",
    "\t\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z,[i1, i2, i3]\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\tdef __init__(self, out_features):\n",
    "\t\tsuper(Flatten, self).__init__()\n",
    "\t\tself.output_dim = out_features\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(-1, self.output_dim)\n",
    "    \n",
    "class Reshape(nn.Module):\n",
    "\tdef __init__(self, out_shape):\n",
    "\t\tsuper(Reshape, self).__init__()\n",
    "\t\tself.out_shape = out_shape\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x.view(-1, *self.out_shape)\n",
    "\n",
    "class Inception_with_auxillary(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_sizes, bottleneck_channels, GAPoutput_size, use_residual):\n",
    "        super(Inception_with_auxillary, self).__init__()\n",
    "        \n",
    "        self.reshape = Reshape(out_shape=(56,1500))\n",
    "        self.block_i = InceptionBlock(\n",
    "            in_channels=56, \n",
    "            n_filters = n_filters, \n",
    "            kernel_sizes = kernel_sizes,\n",
    "            bottleneck_channels = bottleneck_channels,\n",
    "            use_residual = use_residual,\n",
    "            activation=nn.ReLU())\n",
    "        \n",
    "        self.block = InceptionBlock(\n",
    "            in_channels=n_filters*4, \n",
    "            n_filters = n_filters, \n",
    "            kernel_sizes = kernel_sizes,\n",
    "            bottleneck_channels = bottleneck_channels,\n",
    "            use_residual = use_residual,\n",
    "            activation=nn.ReLU())\n",
    "\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = kernel_sizes[1], stride = 5, padding = int(kernel_sizes[1]/2 - 1)),\n",
    "\n",
    "        \n",
    "        self.alt_out = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 3, stride = 5),\n",
    "            nn.Conv1d(in_channels = n_filters*4, out_channels = 64, kernel_size = 112, padding = 'same'),\n",
    "            nn.MaxPool1d(kernel_size = 3, stride = 5),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = 3840, out_features = 400),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features = 400, out_features = 1024),\n",
    "            nn.Linear(in_features = 1024, out_features = 3),\n",
    "            nn.LogSoftmax(-1))\n",
    "        \n",
    "        \n",
    "        self.auxillary_out = nn.Sequential(\n",
    "            \n",
    "                    nn.AdaptiveAvgPool1d(GAPoutput_size),\n",
    "                    Flatten(out_features=n_filters*4*GAPoutput_size),\n",
    "#                     nn.Dropout(0.5),\n",
    "                    nn.Linear(in_features=4*n_filters*GAPoutput_size, out_features=3),\n",
    "                    nn.LogSoftmax())\n",
    "        \n",
    "    def forward(self,input_mat):\n",
    "       \n",
    "        # Initial Layers\n",
    "        resized_input_mat = self.reshape(input_mat)\n",
    "        Inception_out_1 = self.block_i(resized_input_mat)\n",
    "#         Inception_out_2 = self.block(Inception_out_1)\n",
    "#         Inception_out_3 = self.block(Inception_out_2)\n",
    "#         MaxPool_out_1 = self.MPoolM(Inception_out_1)\n",
    "        # Auxillary 1\n",
    "#         aux_1 = self.alt_out(Inception_out_1)\n",
    "#         print(MaxPool_out_1.shape)\n",
    "#         print(Inception_out_1.shape)\n",
    "        aux_1 = self.auxillary_out(Inception_out_1)\n",
    "\n",
    "#         # Deep Blocks 1\n",
    "        Inception_out_2 = self.block(Inception_out_1)\n",
    "#         Inception_out_5 = self.block(Inception_out_4)\n",
    "# #         MaxPool_out_2 = self.MPoolM(Inception_out_2)\n",
    "#         # Auxillary 2\n",
    "        aux_2 = self.auxillary_out(Inception_out_2)\n",
    "\n",
    "        \n",
    "# #         print(Inception_out_2.shape)\n",
    "\n",
    "#         # Deep Blocks 2\n",
    "#         Inception_out_3 = self.block(MaxPool_out_2)\n",
    "#         # Final Out\n",
    "#         main = self.auxillary_out(Inception_out_3)\n",
    "# #         print(Inception_out_3.shape)\n",
    "\n",
    "        main = aux_2\n",
    "        aux_2 = aux_1\n",
    "# #         aux_1 = nn.Softmax(aux_1)\n",
    "# #         aux_2 = nn.Softmax(aux_2)\n",
    "# #         main = nn.Softmax(main)\n",
    "\n",
    "        out_arr = [aux_1, aux_2, main]\n",
    "    \n",
    "        return(out_arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a044f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCB(nn.Module):\n",
    "    def __init__(self, small_kernel, medium_kernel, large_kernel, num_filters):\n",
    "        super(MSCB, self).__init__()\n",
    "        self.name = \"MSCB\"\n",
    "\n",
    "        # Define Small Path\n",
    "        self.convS = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = small_kernel, padding = 'same')\n",
    "        self.MPoolS = nn.MaxPool1d(kernel_size = small_kernel, stride = 5, padding = int(small_kernel/2 - 1))\n",
    "        \n",
    "        # Define Medium Path\n",
    "        self.convM = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = medium_kernel, padding = 'same')\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = medium_kernel, stride = 5, padding = int(medium_kernel/2 - 1))\n",
    "        \n",
    "        # Define Large Path\n",
    "        self.convL = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = large_kernel, padding = 'same')\n",
    "        self.MPoolL = nn.MaxPool1d(kernel_size = large_kernel, stride = 5, padding = int(large_kernel/2 - 1))\n",
    "        \n",
    "        #\n",
    "        self.MPool = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        \n",
    "        #\n",
    "        self.auxMPool = nn.MaxPool1d(kernel_size = 10, stride = 10)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(12)\n",
    "        \n",
    "        self.auxFC = nn.Linear(in_features = 9600, out_features = 3840)\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels = 56, out_channels = 128, kernel_size = 24, padding = 'same')\n",
    "        \n",
    "        #\n",
    "        self.conv2 = nn.Conv1d(in_channels = 3*num_filters + 128, out_channels = 64, kernel_size = 112, padding = 'same')\n",
    "        self.MPool2 = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.conv3 = nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 25, padding = 'same')\n",
    "        self.fc1 = nn.Linear(in_features = 3840, out_features = 400)\n",
    "        self.Dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features = 400, out_features = 1024)\n",
    "        self.fc3 = nn.Linear(in_features = 1024, out_features = 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature Learning Head\n",
    "        \n",
    "        x = torch.moveaxis(x,2,1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        x_S = self.MPoolS(F.relu(self.convS(x)))\n",
    "        x_M = self.MPoolM(F.relu(self.convM(x)))\n",
    "        x_L = self.MPoolL(F.relu(self.convL(x)))\n",
    "        x_O = F.relu(self.conv(self.MPool(x)))\n",
    "        \n",
    "#         x_aux = x_aux.view(-1,3840)\n",
    "        \n",
    "        #Classification Head\n",
    "#         x_aux = F.relu(self.fc1((x_aux)))\n",
    "#         x_aux = self.Dropout(x_aux)\n",
    "#         x_aux = F.relu(self.fc2(x_aux))\n",
    "#         x_aux = self.fc3(x_aux)\n",
    "        \n",
    "        \n",
    "#         print(x_S.shape)\n",
    "#         print(x_M.shape)\n",
    "#         print(x_L.shape)\n",
    "#         print(x_O.shape)\n",
    "        \n",
    "        #\n",
    "        x = torch.cat((x_S,x_M,x_L,x_O),1)\n",
    "        \n",
    "        \n",
    "        x_aux = self.auxMPool(x)\n",
    "#         print(x_aux.shape)\n",
    "        x_aux = x_aux.view(-1,9600)\n",
    "#         print(x_aux.shape)\n",
    "        x_aux = self.auxFC(x_aux)\n",
    "        #Classification Head\n",
    "#         x_aux = x_aux.view(-1,3840)\n",
    "        x_aux = F.relu(self.fc1((x_aux)))\n",
    "        x_aux = self.Dropout(x_aux)\n",
    "        x_aux = F.relu(self.fc2(x_aux))\n",
    "        x_aux = self.fc3(x_aux)\n",
    "#         x_aux = F.softmax(x_aux)\n",
    "\n",
    "        x = self.MPool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flattening\n",
    "        x = x.view(-1,3840)\n",
    "#         print(x.shape)\n",
    "\n",
    "        #Classification Head\n",
    "        x = F.relu(self.fc1((x)))\n",
    "        x = self.Dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "#         x = F.softmax(x)\n",
    "        \n",
    "        return (x, x_aux)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f49083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_with_auxillary(nn.Module):\n",
    "    def __init__(self, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, GAPoutput_size=12, use_residual=True):\n",
    "        super(Inception_with_auxillary, self).__init__()\n",
    "        \n",
    "        self.reshape = Reshape(out_shape=(56,1500))\n",
    "        self.block_i = InceptionBlock(\n",
    "            in_channels=56, \n",
    "            n_filters = n_filters, \n",
    "            kernel_sizes = kernel_sizes,\n",
    "            bottleneck_channels = bottleneck_channels,\n",
    "            use_residual = use_residual,\n",
    "            activation=nn.ReLU())\n",
    "        \n",
    "        self.block = InceptionBlock(\n",
    "            in_channels=n_filters*4, \n",
    "            n_filters = n_filters, \n",
    "            kernel_sizes = kernel_sizes,\n",
    "            bottleneck_channels = bottleneck_channels,\n",
    "            use_residual = use_residual,\n",
    "            activation=nn.ReLU())\n",
    "\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = kernel_sizes[1], stride = 5, padding = int(kernel_sizes[1]/2 - 1)),\n",
    "\n",
    "        \n",
    "        self.alt_out = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size = 3, stride = 5),\n",
    "            nn.Conv1d(in_channels = n_filters*4, out_channels = 64, kernel_size = 112, padding = 'same'),\n",
    "            nn.MaxPool1d(kernel_size = 3, stride = 5),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = 3840, out_features = 400),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features = 400, out_features = 1024),\n",
    "            nn.Linear(in_features = 1024, out_features = 3),\n",
    "            nn.LogSoftmax(-1))\n",
    "        \n",
    "        \n",
    "        self.auxillary_out = nn.Sequential(\n",
    "            \n",
    "                    nn.AdaptiveAvgPool1d(GAPoutput_size),\n",
    "                    Flatten(out_features=n_filters*4*GAPoutput_size),\n",
    "#                     nn.Dropout(0.5),\n",
    "                    nn.Linear(in_features=4*n_filters*GAPoutput_size, out_features=3))\n",
    "        \n",
    "    def forward(self,input_mat):\n",
    "       \n",
    "        # Initial Layers\n",
    "        resized_input_mat = self.reshape(input_mat)\n",
    "        Inception_out_1 = self.block_i(resized_input_mat)\n",
    "#         Inception_out_2 = self.block(Inception_out_1)\n",
    "#         Inception_out_3 = self.block(Inception_out_2)\n",
    "#         MaxPool_out_1 = self.MPoolM(Inception_out_1)\n",
    "        # Auxillary 1\n",
    "#         aux_1 = self.alt_out(Inception_out_1)\n",
    "#         print(MaxPool_out_1.shape)\n",
    "#         print(Inception_out_1.shape)\n",
    "        main = self.auxillary_out(Inception_out_1)\n",
    "\n",
    "# #         # Deep Blocks 1\n",
    "#         Inception_out_2 = self.block(Inception_out_1)\n",
    "# #         Inception_out_5 = self.block(Inception_out_4)\n",
    "# # #         MaxPool_out_2 = self.MPoolM(Inception_out_2)\n",
    "# #         # Auxillary 2\n",
    "#         aux_2 = self.auxillary_out(Inception_out_2)\n",
    "\n",
    "        \n",
    "# # #         print(Inception_out_2.shape)\n",
    "\n",
    "# #         # Deep Blocks 2\n",
    "# #         Inception_out_3 = self.block(MaxPool_out_2)\n",
    "# #         # Final Out\n",
    "# #         main = self.auxillary_out(Inception_out_3)\n",
    "# # #         print(Inception_out_3.shape)\n",
    "\n",
    "#         main = aux_2\n",
    "#         aux_2 = aux_1\n",
    "# # #         aux_1 = nn.Softmax(aux_1)\n",
    "# # #         aux_2 = nn.Softmax(aux_2)\n",
    "# # #         main = nn.Softmax(main)\n",
    "\n",
    "#         out_arr = [aux_1, aux_2, main]\n",
    "    \n",
    "        return(main)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6218a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_mat_gen(net, test_set):\n",
    "    guesses = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in test_set:\n",
    "        batch = (i[0])\n",
    "        num,__,_ = batch.shape\n",
    "\n",
    "        label_batch = (i[1])\n",
    "        for j in range(num):\n",
    "\n",
    "            label = np.array(label_batch)[j]\n",
    "\n",
    "            sample = batch[j,:,:].unsqueeze(0)\n",
    "            sample = sample.cuda()\n",
    "            \n",
    "            \n",
    "            net.eval()\n",
    "            sample = sample.float()\n",
    "            outputs = (net(sample))\n",
    "\n",
    "            outputs = outputs[0]\n",
    "            outputs = torch.Tensor.cpu(outputs)\n",
    "\n",
    "            np_out = outputs.detach().numpy()[0]\n",
    "            guess = np.argmax(np_out, axis=0)\n",
    "            guesses.append(guess)\n",
    "            labels.append(label)\n",
    "            \n",
    "    cf_mat = confusion_matrix(guesses, labels)\n",
    "    \n",
    "    return cf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4b86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 400\n",
    "use_cuda = True\n",
    "early_stop = 3\n",
    "batch_size = 32\n",
    "small_kernel = 1\n",
    "medium_kernel = 3\n",
    "large_kernel = 5\n",
    "min_delta = -0.025\n",
    "min_epochs = 30\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a408af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_filters=32\n",
    "# kernel_sizes=[9,19,39]\n",
    "# bottleneck_channels=32\n",
    "# GAPoutput_size=12\n",
    "# use_residual=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4948e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, d_model, dropout = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        max_len = max(5000, seq_len)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[: , 0 : -1]\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    # Input: seq_len x batch_size x dim, Output: seq_len, batch_size, dim\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Permute(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "class MultitaskTransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, task_type, device, nclasses, seq_len, batch, input_size, emb_size, nhead, nhid, nhid_tar, nhid_task, nlayers, dropout = 0.1):\n",
    "        super(MultitaskTransformerModel, self).__init__()\n",
    "        # from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        \n",
    "        self.trunk_net = nn.Sequential(\n",
    "            nn.Linear(input_size, emb_size),\n",
    "#             nn.BatchNorm1d(batch),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            PositionalEncoding(seq_len, emb_size, dropout),\n",
    "            nn.LayerNorm(emb_size),\n",
    "#             nn.BatchNorm1d(batch)\n",
    "        )\n",
    "        \n",
    "        # encoder_layers = transformer_encoder_class.TransformerEncoderLayer(emb_size, nhead, nhid, out_channel, filter_height, filter_width, dropout)\n",
    "        # encoder_layers = TransformerEncoderLayer(emb_size, nhead, nhid, dropout)\n",
    "        # self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        encoder_layers = TAR_transformer.TransformerEncoderLayer(emb_size, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TAR_transformer.TransformerEncoder(encoder_layers, nlayers, device)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(batch)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(emb_size)\n",
    "        \n",
    "        # Task-aware Reconstruction Layers\n",
    "        self.tar_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, nhid_tar),\n",
    "            nn.BatchNorm1d(batch),\n",
    "            nn.Linear(nhid_tar, nhid_tar),\n",
    "            nn.BatchNorm1d(batch),\n",
    "            nn.Linear(nhid_tar, input_size),\n",
    "        )\n",
    "\n",
    "        if task_type == 'classification':\n",
    "            # Classification Layers\n",
    "            self.class_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "#                 nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Dropout(p = 0.3),\n",
    "                nn.Linear(nhid_task, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "#                 nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Dropout(p = 0.3),\n",
    "                nn.Linear(nhid_task, nclasses)\n",
    "            )\n",
    "        else:\n",
    "            # Regression Layers\n",
    "            self.reg_net = nn.Sequential(\n",
    "                nn.Linear(emb_size, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "                nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Linear(nhid_task, nhid_task),\n",
    "                nn.ReLU(),\n",
    "                Permute(),\n",
    "                nn.BatchNorm1d(batch),\n",
    "                Permute(),\n",
    "                nn.Linear(nhid_task, 1),\n",
    "            )\n",
    "            \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = self.trunk_net(x.permute(2, 0, 1))\n",
    "        x, attn = self.transformer_encoder(x)\n",
    "#         x = self.batch_norm(x)\n",
    "        # x : seq_len x batch x emb_size\n",
    "        output = self.class_net(x[-1])\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb314c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCB(nn.Module):\n",
    "    def __init__(self, small_kernel, medium_kernel, large_kernel, num_filters):\n",
    "        super(MSCB, self).__init__()\n",
    "        self.name = \"MSCB\"\n",
    "\n",
    "        # Define Small Path\n",
    "        self.convS = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = small_kernel, padding = 'same')\n",
    "        self.MPoolS = nn.MaxPool1d(kernel_size = small_kernel, stride = 5, padding = int(small_kernel/2 - 1))\n",
    "        \n",
    "        # Define Medium Path\n",
    "        self.convM = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = medium_kernel, padding = 'same')\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = medium_kernel, stride = 5, padding = int(medium_kernel/2 - 1))\n",
    "        \n",
    "        # Define Large Path\n",
    "        self.convL = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = large_kernel, padding = 'same')\n",
    "        self.MPoolL = nn.MaxPool1d(kernel_size = large_kernel, stride = 5, padding = int(large_kernel/2 - 1))\n",
    "        \n",
    "        # MPool first\n",
    "        self.MPool = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.conv = nn.Conv1d(in_channels = 56, out_channels = 128, kernel_size = 24, padding = 'same')\n",
    "        \n",
    "        # Post Concatenation\n",
    "        self.conv2 = nn.Conv1d(in_channels = 3*num_filters + 128, out_channels = 64, kernel_size = 112, padding = 'same')\n",
    "        self.MPool2 = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.fc1 = nn.Linear(in_features = 256, out_features = 400)\n",
    "        self.Dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features = 400, out_features = 1024)\n",
    "        self.fc3 = nn.Linear(in_features = 1024, out_features = 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Feature Learning Head\n",
    "        \n",
    "        # Reshape Tensor\n",
    "        x = torch.moveaxis(x,2,1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        # Parrallel Convolution Pathways\n",
    "        x_S = self.MPoolS(F.relu(self.convS(x)))\n",
    "        x_M = self.MPoolM(F.relu(self.convM(x)))\n",
    "        x_L = self.MPoolL(F.relu(self.convL(x)))\n",
    "        x_O = F.relu(self.conv(self.MPool(x)))\n",
    "        \n",
    "        # Post Concatenation\n",
    "        x = torch.cat((x_S,x_M,x_L,x_O),1)\n",
    "        x = self.MPool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flattening\n",
    "        x = x.view(-1,256)\n",
    "\n",
    "        #Classification Head\n",
    "        x = F.relu(self.fc1((x)))\n",
    "        x = self.Dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a10f5eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"classification\"\n",
    "device = \"cuda\"\n",
    "nclasses = 3\n",
    "seq_len = 1500\n",
    "batch = 64\n",
    "input_size = 1500\n",
    "emb_size = 512\n",
    "nhead = 8\n",
    "nhid = 64\n",
    "nhid_tar = 1024\n",
    "nhid_task = 128\n",
    "nlayers = 2\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 80\n",
    "use_cuda = True\n",
    "early_stop = 3\n",
    "min_delta = -0.025\n",
    "min_epochs = 30\n",
    "num_workers = 8\n",
    "\n",
    "n_filters = 64\n",
    "small_kernel = 1\n",
    "medium_kernel = 3\n",
    "large_kernel = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f91a1cd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 2kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EBB790>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\.conda\\envs\\Aseem\\lib\\site-packages\\torch\\nn\\modules\\conv.py:303: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:883.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 2kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EA1EE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 2kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EA1E80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 3kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EDE670>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 3kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EDF3970>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 3kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EDF34F0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 4kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EBBE20>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 4kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EA1E80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 4kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EA1760>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 5kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0A910>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 5kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0A340>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 5kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0A160>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 6kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EDEC40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 6kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EDFE20>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 6kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA90BBDB50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 7kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AB08C73F40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 7kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AB08C73E80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 7kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0AF40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 8kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0AC70>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 8kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0A5B0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 8kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EE0A460>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 9kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AB08C73A90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 9kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EDF3880>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 9kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA8EDF3A00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['final', 'final', 'final']\n",
      "weights\n",
      "Rat 10kernels_1_3_5Fold_1_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AB08C73880>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 10kernels_1_3_5Fold_2_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AAC9EA16D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "Rat 10kernels_1_3_5Fold_3_final\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AA90BBDFA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2294328818.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_25728\\2758336073.py:11: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  return(num/den)\n"
     ]
    }
   ],
   "source": [
    "count2 = 0\n",
    "for j in [2,3,4,5,6,7,8,9,10]:\n",
    "    rat = \"Rat \" + str(j)\n",
    "#     print(rat)\n",
    "\n",
    "    base_dir = \"M:\\Peripheral Nerve Studies\\MCC Projects\\Aseem G\\Models\\Pytorch\\Data\\Spike 100\\Minimum Dataset\\\\\" + rat + \"\\\\\"\n",
    "    fold1_dir = base_dir + \"Fold1\"\n",
    "    fold2_dir = base_dir + \"Fold2\"\n",
    "    fold3_dir = base_dir + \"Fold3\"\n",
    "    test_dir = base_dir + \"Test\"\n",
    "    train_set_1, valid_set_1, train_set_2, valid_set_2, train_set_3, valid_set_3, test_set = three_fold_cross_sets(base_dir,fold1_dir,fold2_dir,fold3_dir,test_dir, batch_size = 32)\n",
    "\n",
    "    # [25, \"final\", \"final\"]\n",
    "    epochs_list = [[\"final\",\"final\",\"final\"], [\"final\",\"final\",\"final\"], [\"final\",\"final\",\"final\"], [\"final\",\"final\",\"final\"], [\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"],[\"final\",\"final\",\"final\"]]\n",
    "    print(len(epochs_list))\n",
    "    epochs = epochs_list[count2]\n",
    "    print(epochs)\n",
    "    \n",
    "    count2 += 1\n",
    "    \n",
    "    small_kernel = 1\n",
    "    medium_kernel = 3\n",
    "    large_kernel = 5\n",
    "\n",
    "    one_CNN = MSCB(small_kernel, medium_kernel, large_kernel, n_filters)\n",
    "    one_CNN = one_CNN.cuda()\n",
    "    one_CNN = one_CNN.eval()\n",
    "    count = 1\n",
    "    \n",
    "    cfmats = []\n",
    "    t_cfmats = []\n",
    "\n",
    "    for i in epochs:\n",
    "#         print(rat)\n",
    "        path = \"M:\\Peripheral Nerve Studies\\MCC Projects\\Aseem G\\Models\\Pytorch\\model_checkpoints\\\\\" + rat + \"\\\\March 20 MSCB 100-2\\\\\"\n",
    "        weights = rat + \"kernels_1_3_5Fold_\" + str(count) + \"_epochno_\" + str(i)\n",
    "        if i == \"final\":\n",
    "            weights = rat + \"kernels_1_3_5Fold_\" + str(count) + \"_final\"\n",
    "\n",
    "        print(\"weights\")\n",
    "        print(weights)\n",
    "        weights_path = path + weights\n",
    "#         print(weights_path)\n",
    "#         state = torch.load(weights_path, map_location=torch.device('cpu') )\n",
    "        state = torch.load(weights_path, map_location=torch.device('cuda'))\n",
    "        one_CNN.load_state_dict(state)\n",
    "\n",
    "        valid_set = [valid_set_1, valid_set_2, valid_set_3][count - 1]\n",
    "\n",
    "        print(valid_set)\n",
    "        \n",
    "#         valid_set = valid_set.cuda()\n",
    "#         test_set = test_set.cuda()\n",
    "\n",
    "        cf_mat = cf_mat_gen(one_CNN, valid_set)\n",
    "        f = open(path + \"Best.txt\", \"a\")\n",
    "        f.write(\"\\nFold \" + str(count) + \"\\n\")\n",
    "        f.write(\"Best Epoch =\" + str(i) + \"\\n\")\n",
    "        f.write(str(cf_mat))\n",
    "        f.write(\"\\nValid Accuracy: \" + str(accuracy(cf_mat)) + \"%\\n\")\n",
    "        f.write(\"Valid Macro F1 Score: \" + str(macro_f1(cf_mat)))\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "        cfmats.append(accuracy(cf_mat))\n",
    "        cfmats.append(macro_f1(cf_mat))\n",
    "        \n",
    "        f = open(path + \"Best.txt\", \"a\")\n",
    "        cf_mat = cf_mat_gen(one_CNN, test_set)\n",
    "        f.write(str(cf_mat))\n",
    "        f.write(\"\\nTest Accuracy: \" + str(accuracy(cf_mat)) + \"%\\n\")\n",
    "        f.write(\"Test Macro F1 Score: \" + str(macro_f1(cf_mat)))\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "        count += 1\n",
    "        \n",
    "        t_cfmats.append(accuracy(cf_mat))\n",
    "        t_cfmats.append(macro_f1(cf_mat))\n",
    "    \n",
    "    string = \"\"\n",
    "    for i in cfmats:\n",
    "        if i <= 1:\n",
    "            string += str(i) + \"\\t\"\n",
    "        elif i > 1:\n",
    "            string += str(i) + \"%\\t\"\n",
    "\n",
    "    f = open(path + \"Best.txt\", \"a\")\n",
    "    f.write(\"\\nValidation Accuracy\\tValid Macro F1 Score\\tValidation Accuracy\\tValid Macro F1 Score\\tValidation Accuracy\\tValid Macro F1 Score\\n\")\n",
    "    f.write(string)\n",
    "    f.close()\n",
    "    \n",
    "    string = \"\"\n",
    "    for i in t_cfmats:\n",
    "        if i <= 1:\n",
    "            string += str(i) + \"\\t\"\n",
    "        elif i > 1:\n",
    "            string += str(i) + \"%\\t\"\n",
    "\n",
    "    f = open(path + \"Best.txt\", \"a\")\n",
    "    f.write(\"\\nTest Accuracy\\tTest Macro F1 Score\\tTest Accuracy\\tTest Macro F1 Score\\tTest Accuracy\\tTest Macro F1 Score\\n\")\n",
    "    f.write(string)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82efb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_CNN = MSCB(n_filters, small_kernel, medium_kernel, large_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f15105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSCB(\n",
       "  (convS): Conv1d(56, 5, kernel_size=(64,), stride=(1,), padding=same)\n",
       "  (MPoolS): MaxPool1d(kernel_size=64, stride=5, padding=31, dilation=1, ceil_mode=False)\n",
       "  (convM): Conv1d(56, 5, kernel_size=(1,), stride=(1,), padding=same)\n",
       "  (MPoolM): MaxPool1d(kernel_size=1, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (convL): Conv1d(56, 5, kernel_size=(3,), stride=(1,), padding=same)\n",
       "  (MPoolL): MaxPool1d(kernel_size=3, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (MPool): MaxPool1d(kernel_size=3, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv): Conv1d(56, 128, kernel_size=(24,), stride=(1,), padding=same)\n",
       "  (conv2): Conv1d(143, 64, kernel_size=(112,), stride=(1,), padding=same)\n",
       "  (MPool2): MaxPool1d(kernel_size=3, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3840, out_features=400, bias=True)\n",
       "  (Dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=400, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0734a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "811ca72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[638,   1,   1],\n",
       "       [266, 865,   1],\n",
       "       [  0,  38, 902]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf54af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
